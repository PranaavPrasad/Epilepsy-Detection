{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b346976",
   "metadata": {},
   "source": [
    "# Phase 2: EEG Data Preprocessing\n",
    "## CHB-MIT Dataset - Data Cleaning and Feature Extraction\n",
    "\n",
    "This notebook preprocesses EEG data from selected subjects (5-6) for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4085d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7a7be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# EEG processing\n",
    "import mne\n",
    "from scipy import signal\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Data storage\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7479c",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e060c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: c:\\Users\\Pranaav_Prasad\\OneDrive\\Desktop\\Projects\\Epilepsy-Detection\n",
      "Selected Subjects: ['chb01', 'chb02', 'chb03', 'chb04', 'chb05', 'chb24']\n",
      "Sampling Rate: 256 Hz → 128 Hz (downsampled 2x)\n",
      "Window: 4s with 2s overlap (512 samples)\n",
      "Memory savings: ~50% from downsampling + 50% from float32 = ~75% total reduction\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path(r\"c:\\Users\\Pranaav_Prasad\\OneDrive\\Desktop\\Projects\\Epilepsy-Detection\")\n",
    "RAW_DATA_DIR = BASE_DIR / \"data\" / \"raw\" / \"chb-mit-scalp-eeg-database-1.0.0\"\n",
    "PROCESSED_DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# Selected subjects (top 6 with most seizures)\n",
    "SELECTED_SUBJECTS = ['chb01', 'chb02', 'chb03', 'chb04', 'chb05', 'chb24']\n",
    "\n",
    "# EEG parameters - OPTIMIZED FOR DISK SPACE\n",
    "ORIGINAL_SAMPLING_RATE = 256  # Hz (original)\n",
    "SAMPLING_RATE = 128  # Hz (downsampled to save space - still adequate for seizure detection)\n",
    "DOWNSAMPLE_FACTOR = ORIGINAL_SAMPLING_RATE // SAMPLING_RATE\n",
    "WINDOW_SIZE = 4  # seconds\n",
    "OVERLAP = 2  # seconds\n",
    "N_SAMPLES_PER_WINDOW = SAMPLING_RATE * WINDOW_SIZE  # 512 samples (was 1024)\n",
    "\n",
    "# Frequency bands\n",
    "FREQ_BANDS = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'gamma': (30, 50)\n",
    "}\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Selected Subjects: {SELECTED_SUBJECTS}\")\n",
    "print(f\"Sampling Rate: {ORIGINAL_SAMPLING_RATE} Hz → {SAMPLING_RATE} Hz (downsampled {DOWNSAMPLE_FACTOR}x)\")\n",
    "print(f\"Window: {WINDOW_SIZE}s with {OVERLAP}s overlap ({N_SAMPLES_PER_WINDOW} samples)\")\n",
    "print(f\"Memory savings: ~50% from downsampling + 50% from float32 = ~75% total reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65eb44",
   "metadata": {},
   "source": [
    "## 3. Parse Summary Files for Seizure Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab23807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chb01: 7 seizure events\n",
      "chb02: 3 seizure events\n",
      "chb03: 7 seizure events\n",
      "chb04: 2 seizure events\n",
      "chb05: 5 seizure events\n",
      "chb24: 16 seizure events\n",
      "\n",
      "✓ Loaded seizure information\n"
     ]
    }
   ],
   "source": [
    "def parse_summary_file(summary_path):\n",
    "    \"\"\"Extract seizure information from subject summary file.\"\"\"\n",
    "    seizure_info = []\n",
    "    \n",
    "    with open(summary_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        current_file = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'File Name:' in line:\n",
    "                current_file = line.split(':')[1].strip()\n",
    "            elif 'Seizure Start Time:' in line and current_file:\n",
    "                start_time = int(line.split(':')[1].strip().split()[0])\n",
    "                # Find end time in next line\n",
    "                if i + 1 < len(lines) and 'Seizure End Time:' in lines[i + 1]:\n",
    "                    end_time = int(lines[i + 1].split(':')[1].strip().split()[0])\n",
    "                    seizure_info.append({\n",
    "                        'file': current_file,\n",
    "                        'start': start_time,\n",
    "                        'end': end_time\n",
    "                    })\n",
    "    \n",
    "    return seizure_info\n",
    "\n",
    "# Load seizure information for selected subjects\n",
    "seizure_data = {}\n",
    "for subject in SELECTED_SUBJECTS:\n",
    "    summary_file = RAW_DATA_DIR / subject / f\"{subject}-summary.txt\"\n",
    "    if summary_file.exists():\n",
    "        seizure_data[subject] = parse_summary_file(summary_file)\n",
    "        print(f\"{subject}: {len(seizure_data[subject])} seizure events\")\n",
    "\n",
    "print(f\"\\n✓ Loaded seizure information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109ca50",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee1e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing functions defined (with downsampling)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_eeg(raw_data, sampling_rate=256, target_rate=128):\n",
    "    \"\"\"Apply bandpass filter, downsample, and normalization.\"\"\"\n",
    "    # Bandpass filter (0.5-50 Hz)\n",
    "    nyquist = sampling_rate / 2\n",
    "    low, high = 0.5 / nyquist, 50 / nyquist\n",
    "    b, a = signal.butter(4, [low, high], btype='band')\n",
    "    filtered = signal.filtfilt(b, a, raw_data, axis=1)\n",
    "    \n",
    "    # Downsample to save space (256 Hz -> 128 Hz)\n",
    "    if sampling_rate > target_rate:\n",
    "        downsample_factor = sampling_rate // target_rate\n",
    "        filtered = filtered[:, ::downsample_factor]\n",
    "    \n",
    "    # Normalization (z-score)\n",
    "    mean = np.mean(filtered, axis=1, keepdims=True)\n",
    "    std = np.std(filtered, axis=1, keepdims=True)\n",
    "    normalized = (filtered - mean) / (std + 1e-8)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def extract_spectral_features(data, sampling_rate, freq_bands):\n",
    "    \"\"\"Extract power in frequency bands.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for ch_data in data:\n",
    "        freqs, psd = signal.welch(ch_data, fs=sampling_rate, nperseg=256)\n",
    "        \n",
    "        ch_features = []\n",
    "        for band_name, (low, high) in freq_bands.items():\n",
    "            idx = np.logical_and(freqs >= low, freqs <= high)\n",
    "            band_power = np.mean(psd[idx])\n",
    "            ch_features.append(band_power)\n",
    "        \n",
    "        features.extend(ch_features)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_statistical_features(data):\n",
    "    \"\"\"Extract statistical features from each channel.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for ch_data in data:\n",
    "        features.extend([\n",
    "            np.mean(ch_data),\n",
    "            np.std(ch_data),\n",
    "            skew(ch_data),\n",
    "            kurtosis(ch_data),\n",
    "            np.max(ch_data) - np.min(ch_data)  # range\n",
    "        ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "print(\"✓ Preprocessing functions defined (with downsampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9a1e9",
   "metadata": {},
   "source": [
    "## 5. Process EEG Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c29308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EDF processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_edf_file(edf_path, seizure_times=None):\n",
    "    \"\"\"Process single EDF file and extract windows.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(str(edf_path), preload=True, verbose=False)\n",
    "    data = raw.get_data()\n",
    "    \n",
    "    # Preprocess\n",
    "    data = preprocess_eeg(data, SAMPLING_RATE)\n",
    "    \n",
    "    # Create sliding windows\n",
    "    n_channels, n_samples = data.shape\n",
    "    step_size = (WINDOW_SIZE - OVERLAP) * SAMPLING_RATE\n",
    "    \n",
    "    windows = []\n",
    "    labels = []\n",
    "    \n",
    "    for start_idx in range(0, n_samples - N_SAMPLES_PER_WINDOW, step_size):\n",
    "        end_idx = start_idx + N_SAMPLES_PER_WINDOW\n",
    "        window_data = data[:, start_idx:end_idx]\n",
    "        \n",
    "        # Determine label\n",
    "        window_time_start = start_idx / SAMPLING_RATE\n",
    "        window_time_end = end_idx / SAMPLING_RATE\n",
    "        \n",
    "        is_seizure = False\n",
    "        if seizure_times:\n",
    "            for sz in seizure_times:\n",
    "                if (window_time_start >= sz['start'] and window_time_start < sz['end']) or \\\n",
    "                   (window_time_end > sz['start'] and window_time_end <= sz['end']):\n",
    "                    is_seizure = True\n",
    "                    break\n",
    "        \n",
    "        windows.append(window_data)\n",
    "        labels.append(1 if is_seizure else 0)\n",
    "    \n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "print(\"✓ EDF processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba8c18",
   "metadata": {},
   "source": [
    "## 6. Process All Selected Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a7e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FULL dataset with memory-efficient streaming...\\n\n",
      "Selected subjects: ['chb01', 'chb02', 'chb03', 'chb04', 'chb05', 'chb24']\n",
      "Processing ALL .edf files from each subject\\n\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e9b40a0861410182c3ce6f18131140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subjects:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nchb01: Found 42 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29312ee124764c458b6549cce246ea88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chb01:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nchb02: Found 36 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af031a9a1ca49948e5d00dea50f4524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chb02:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nchb03: Found 38 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3fe4f762554e22b83e1a6ce04c4792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chb03:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nchb04: Found 42 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f77c18aac6644fab2f4ebd635d44261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chb04:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n  ⚠ Error processing chb04_07.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_08.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_09.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_10.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_09.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_10.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_11.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_12.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_11.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_12.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_13.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_14.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_13.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_14.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_15.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_16.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_15.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_16.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_17.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_18.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_17.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_18.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_19.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_21.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_19.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_21.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_22.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_23.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_22.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_23.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_24.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_25.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_24.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_25.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_26.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_27.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_26.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_27.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_28.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_29.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_28.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_29.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_30.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_31.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_30.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_31.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_32.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_33.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_32.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_33.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_34.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_35.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_34.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_35.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_36.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_37.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_36.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_37.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_38.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_39.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_38.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_39.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_40.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_41.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_40.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_41.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_42.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_43.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\nchb05: Found 39 files\n",
      "\\n  ⚠ Error processing chb04_42.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\n  ⚠ Error processing chb04_43.edf: Can't broadcast (149, 24, 512) -> (149, 23, 512)\n",
      "\\nchb05: Found 39 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2596dcc70624a6685224e512b58df9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chb05:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nchb24: Found 22 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0eb44016a949d5804c596585e019ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chb24:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n======================================================================\n",
      "✓ FULL DATASET PROCESSING COMPLETE\n",
      "======================================================================\n",
      "Total files processed: 183\n",
      "Total windows: 351,815\n",
      "Seizure windows: 1,116 (0.32%)\n",
      "Normal windows: 350,699 (99.68%)\n",
      "======================================================================\n",
      "\\nSaved to: c:\\Users\\Pranaav_Prasad\\OneDrive\\Desktop\\Projects\\Epilepsy-Detection\\data\\processed\\preprocessed_data.h5\n",
      "File size: 13781.49 MB\n"
     ]
    }
   ],
   "source": [
    "# Memory-efficient processing: Stream data directly to HDF5\n",
    "# Process ALL files from ALL 6 selected subjects\n",
    "# Key optimizations:\n",
    "# 1. Process one file at a time (never load all data into RAM)\n",
    "# 2. Use float32 instead of float64 (50% memory reduction)\n",
    "# 3. Process in chunks and write directly to HDF5\n",
    "# 4. Use compression in HDF5 to reduce file size\n",
    "\n",
    "output_file = PROCESSED_DATA_DIR / \"preprocessed_data.h5\"\n",
    "\n",
    "print(\"Processing FULL dataset with memory-efficient streaming...\\\\n\")\n",
    "print(f\"Selected subjects: {SELECTED_SUBJECTS}\")\n",
    "print(f\"Processing ALL .edf files from each subject\\\\n\")\n",
    "\n",
    "# Initialize counters\n",
    "total_windows = 0\n",
    "total_seizure = 0\n",
    "total_files = 0\n",
    "\n",
    "# Create HDF5 file\n",
    "with h5py.File(output_file, 'w') as hdf:\n",
    "    first_write = True\n",
    "    \n",
    "    # Process ALL selected subjects\n",
    "    for subject in tqdm(SELECTED_SUBJECTS, desc=\"Subjects\"):\n",
    "        subject_path = RAW_DATA_DIR / subject\n",
    "        edf_files = sorted([f for f in subject_path.glob(\"*.edf\") if not f.name.endswith('+')])\n",
    "        \n",
    "        seizure_info = seizure_data.get(subject, [])\n",
    "        \n",
    "        print(f\"\\\\n{subject}: Found {len(edf_files)} files\")\n",
    "        \n",
    "        # Process ALL files for this subject\n",
    "        for edf_file in tqdm(edf_files, desc=f\"  {subject}\", leave=False):\n",
    "            try:\n",
    "                # Find seizure times for this file\n",
    "                file_seizures = [s for s in seizure_info if s['file'] == edf_file.name]\n",
    "                \n",
    "                # Load data WITHOUT preloading (memory efficient)\n",
    "                raw = mne.io.read_raw_edf(str(edf_file), preload=False, verbose=False)\n",
    "                \n",
    "                # Process in chunks to avoid loading all data at once\n",
    "                chunk_duration = 300  # 5 minutes per chunk\n",
    "                total_duration = raw.times[-1]\n",
    "                n_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "                \n",
    "                for chunk_idx in range(n_chunks):\n",
    "                    start_time = chunk_idx * chunk_duration\n",
    "                    end_time = min((chunk_idx + 1) * chunk_duration, total_duration)\n",
    "                    \n",
    "                    # Load only this chunk\n",
    "                    start_sample = int(start_time * ORIGINAL_SAMPLING_RATE)\n",
    "                    stop_sample = int(end_time * ORIGINAL_SAMPLING_RATE)\n",
    "                    \n",
    "                    data = raw.get_data(start=start_sample, stop=stop_sample)\n",
    "                    \n",
    "                    # Preprocess with downsampling and convert to float32 (memory efficient)\n",
    "                    data = preprocess_eeg(data, ORIGINAL_SAMPLING_RATE, SAMPLING_RATE).astype(np.float32)\n",
    "                    \n",
    "                    # Create windows for this chunk\n",
    "                    n_channels, n_samples = data.shape\n",
    "                    step_size = int((WINDOW_SIZE - OVERLAP) * SAMPLING_RATE)\n",
    "                    \n",
    "                    windows_list = []\n",
    "                    labels_list = []\n",
    "                    \n",
    "                    for start_idx in range(0, n_samples - N_SAMPLES_PER_WINDOW + 1, step_size):\n",
    "                        end_idx = start_idx + N_SAMPLES_PER_WINDOW\n",
    "                        window_data = data[:, start_idx:end_idx]\n",
    "                        \n",
    "                        # Calculate absolute time for labeling\n",
    "                        window_time_start = start_time + (start_idx / SAMPLING_RATE)\n",
    "                        window_time_end = start_time + (end_idx / SAMPLING_RATE)\n",
    "                        \n",
    "                        # Determine label\n",
    "                        is_seizure = False\n",
    "                        if file_seizures:\n",
    "                            for sz in file_seizures:\n",
    "                                if (window_time_start >= sz['start'] and window_time_start < sz['end']) or \\\n",
    "                                   (window_time_end > sz['start'] and window_time_end <= sz['end']):\n",
    "                                    is_seizure = True\n",
    "                                    break\n",
    "                        \n",
    "                        windows_list.append(window_data)\n",
    "                        labels_list.append(1 if is_seizure else 0)\n",
    "                    \n",
    "                    # Write this chunk to HDF5 immediately\n",
    "                    if windows_list:\n",
    "                        windows = np.array(windows_list, dtype=np.float32)\n",
    "                        labels = np.array(labels_list, dtype=np.int8)\n",
    "                        \n",
    "                        if first_write:\n",
    "                            maxshape_X = (None, windows.shape[1], windows.shape[2])\n",
    "                            maxshape_y = (None,)\n",
    "                            \n",
    "                            hdf.create_dataset('X', data=windows, maxshape=maxshape_X, \n",
    "                                             chunks=(min(100, windows.shape[0]), windows.shape[1], windows.shape[2]), \n",
    "                                             compression='gzip', compression_opts=4)\n",
    "                            hdf.create_dataset('y', data=labels, maxshape=maxshape_y,\n",
    "                                             chunks=True, compression='gzip', compression_opts=4)\n",
    "                            first_write = False\n",
    "                        else:\n",
    "                            current_size = hdf['X'].shape[0]\n",
    "                            hdf['X'].resize((current_size + windows.shape[0]), axis=0)\n",
    "                            hdf['X'][current_size:] = windows\n",
    "                            \n",
    "                            hdf['y'].resize((current_size + labels.shape[0]), axis=0)\n",
    "                            hdf['y'][current_size:] = labels\n",
    "                        \n",
    "                        total_windows += len(windows)\n",
    "                        total_seizure += np.sum(labels)\n",
    "                    \n",
    "                    # Clear memory\n",
    "                    del data, windows_list, labels_list\n",
    "                    if 'windows' in locals():\n",
    "                        del windows, labels\n",
    "                \n",
    "                total_files += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\\\n  ⚠ Error processing {edf_file.name}: {str(e)[:80]}\")\n",
    "                continue\n",
    "    \n",
    "    # Save metadata\n",
    "    if 'X' in hdf:\n",
    "        hdf.attrs['n_samples'] = hdf['X'].shape[0]\n",
    "        hdf.attrs['n_channels'] = hdf['X'].shape[1]\n",
    "        hdf.attrs['window_size'] = WINDOW_SIZE\n",
    "        hdf.attrs['sampling_rate'] = SAMPLING_RATE\n",
    "        hdf.attrs['subjects'] = ','.join(SELECTED_SUBJECTS)\n",
    "        hdf.attrs['total_files_processed'] = total_files\n",
    "\n",
    "if total_windows > 0:\n",
    "    print(f\"\\\\n{'='*70}\")\n",
    "    print(f\"✓ FULL DATASET PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Total windows: {total_windows:,}\")\n",
    "    print(f\"Seizure windows: {total_seizure:,} ({total_seizure/total_windows*100:.2f}%)\")\n",
    "    print(f\"Normal windows: {total_windows - total_seizure:,} ({(total_windows-total_seizure)/total_windows*100:.2f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\\\nSaved to: {output_file}\")\n",
    "    print(f\"File size: {output_file.stat().st_size / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"\\\\n✗ No data processed - check for errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50f20f",
   "metadata": {},
   "source": [
    "## 7. Balance Dataset (Handle Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate classes\n",
    "seizure_indices = np.where(y == 1)[0]\n",
    "normal_indices = np.where(y == 0)[0]\n",
    "\n",
    "print(f\"Original - Seizure: {len(seizure_indices)}, Normal: {len(normal_indices)}\")\n",
    "\n",
    "# Undersample normal class to balance (or use a ratio like 1:3)\n",
    "n_normal_samples = min(len(seizure_indices) * 3, len(normal_indices))\n",
    "normal_indices_sampled = resample(normal_indices, n_samples=n_normal_samples, random_state=42)\n",
    "\n",
    "# Combine\n",
    "balanced_indices = np.concatenate([seizure_indices, normal_indices_sampled])\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "X_balanced = X[balanced_indices]\n",
    "y_balanced = y[balanced_indices]\n",
    "\n",
    "print(f\"Balanced - Total: {len(X_balanced)}\")\n",
    "print(f\"Seizure: {np.sum(y_balanced)} ({np.sum(y_balanced)/len(y_balanced)*100:.2f}%)\")\n",
    "print(f\"Normal: {len(y_balanced) - np.sum(y_balanced)} ({(len(y_balanced)-np.sum(y_balanced))/len(y_balanced)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83593ca",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825cf165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying c:\\Users\\Pranaav_Prasad\\OneDrive\\Desktop\\Projects\\Epilepsy-Detection\\data\\processed\\preprocessed_data.h5...\n",
      "\n",
      "Available datasets: ['X', 'y']\n",
      "Available attributes: ['n_channels', 'n_samples', 'sampling_rate', 'subjects', 'total_files_processed', 'window_size']\n",
      "\n",
      "Data shapes:\n",
      "  X: (357179, 23, 512) (samples, channels, time_points)\n",
      "  y: (357179,) (samples,)\n",
      "\n",
      "Class distribution:\n",
      "  Seizure (1): 1116 (0.31%)\n",
      "  Normal (0): 356063 (99.69%)\n",
      "\n",
      "Data statistics:\n",
      "  X dtype: float32\n",
      "Data shapes:\n",
      "  X: (357179, 23, 512) (samples, channels, time_points)\n",
      "  y: (357179,) (samples,)\n",
      "\n",
      "Class distribution:\n",
      "  Seizure (1): 1116 (0.31%)\n",
      "  Normal (0): 356063 (99.69%)\n",
      "\n",
      "Data statistics:\n",
      "  X dtype: float32\n",
      "  X range: [-65.408, 59.200]\n",
      "  X range: [-65.408, 59.200]\n",
      "  X mean: -0.000\n",
      "  X mean: -0.000\n",
      "  X std: 0.992\n",
      "\n",
      "Preprocessed data verified successfully!\n",
      "  X std: 0.992\n",
      "\n",
      "Preprocessed data verified successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved data\n",
    "output_file = PROCESSED_DATA_DIR / \"preprocessed_data.h5\"\n",
    "print(f\"Verifying {output_file}...\\n\")\n",
    "\n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    print(f\"Available datasets: {list(f.keys())}\")\n",
    "    print(f\"Available attributes: {list(f.attrs.keys())}\\n\")\n",
    "    \n",
    "    if 'X' in f:\n",
    "        X_data = f['X'][:]\n",
    "        y_data = f['y'][:]\n",
    "        \n",
    "        print(f\"Data shapes:\")\n",
    "        print(f\"  X: {X_data.shape} (samples, channels, time_points)\")\n",
    "        print(f\"  y: {y_data.shape} (samples,)\")\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        print(f\"  Seizure (1): {np.sum(y_data == 1)} ({np.sum(y_data == 1) / len(y_data) * 100:.2f}%)\")\n",
    "        print(f\"  Normal (0): {np.sum(y_data == 0)} ({np.sum(y_data == 0) / len(y_data) * 100:.2f}%)\")\n",
    "        print(f\"\\nData statistics:\")\n",
    "        print(f\"  X dtype: {X_data.dtype}\")\n",
    "        print(f\"  X range: [{X_data.min():.3f}, {X_data.max():.3f}]\")\n",
    "        print(f\"  X mean: {X_data.mean():.3f}\")\n",
    "        print(f\"  X std: {X_data.std():.3f}\")\n",
    "        \n",
    "        print(f\"\\nPreprocessed data verified successfully!\")\n",
    "    else:\n",
    "        print(\"ERROR: No data found in HDF5 file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ede41",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "**Preprocessing Complete:**\n",
    "- Filtered EEG signals (0.5-50 Hz bandpass)\n",
    "- Normalized data (z-score)\n",
    "- Created sliding windows (4s with 2s overlap)\n",
    "- Labeled windows as seizure/normal\n",
    "- Balanced dataset\n",
    "- Saved processed data for training\n",
    "\n",
    "**Next: Phase 3 - Model Training**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
